# ğŸ”¤ Neural Machine Translation (English â†” French)

This project implements a custom neural machine translation (NMT) system using a sequence-to-sequence architecture with attention.

---

## ğŸš€ Features

- Encoder-Decoder model with Luong-style attention
- Beam search decoding
- BLEU score evaluation on validation/test sets
- Clean modular PyTorch code
- Fully configurable training + inference pipeline

---

## ğŸ§  Model Architecture

- **Encoder**: LSTM with embeddings
- **Attention**: Bahdanauâ€™s additive attention
- **Decoder**: LSTM with context vector + output projection
- **Training**: Teacher forcing + CrossEntropyLoss

---

## ğŸ“Š Dataset

- Source: [Tatoeba English-French sentence pairs](https://tatoeba.org/en/downloads)
- Preprocessed to `train.csv`, `val.csv`, `test.csv` in `data/processed/`

---

## ğŸ‹ï¸ Training

```bash
python scripts/train.py
```
